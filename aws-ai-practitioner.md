![](assets/aws-ai-cloud-practitioner-banner.png)
# AWS AI Practitioner Study Guide
This page contains my notes for the new AWS AI practitioner certification. The goal is to cover the high level objectives you’ll need to know for the exam. If you would like a specific deep dive on anything, I would recommend referencing additional material, including the study resources I recommend below.



## Study Resources
While I personally already have a lot of experience working with AI on AWS, I’m still glad I opted to check out these study materials to refine my knowledge on pieces I don’t work with as often. Specifically, I leveraged the following two resources on Udemy by Stephen Maarek, who has produced a number of great AWS courses that I’ve taken historically for other AWS exams.

- [**Video Course**](https://www.udemy.com/share/10bvuD3@1rKUZUHRjAzbnkdoKBdveLxMgZEnHJX0NBmYNP7668Q3DtlXfdKGh8Omk6qOdsY=/): This course contains a number of videos about each objective covered in the exam.
- [**Practice Exams**](https://www.udemy.com/share/10bvwn3@J-lVKxcoTQj-kkUgkwxUnT9dpQMk5vrDLG0AniHwoeut-PNUcU-8_W6QTXL8l90=/): Complementary to the video course above, these practice tests will allow you to hone your knowledge with realistic questions that you may encounter on the exam itself. The practice exams also explain the intuition behind why a question’s correct answer is what it is.



## Study Notes
We’ll now move into my personal study notes. The notes will largely correlate to how the sections are laid out in the video course referenced above, but it is not particularly necessary to review the course to make sense of my notes. My notes also come from my own personal experience working with these services on AWS.



### Artificial Intelligence (AI) & Machine Learning (ML)
In this section, we’ll cover an introduction to just what is artificial intelligence (AI) and machine learning (ML). If you want a deeper dive on any of these concepts, I would again recommend the Udemy course linked above. Active practitioners in the AI/ML space can probably skip this section. (Note: I’m deviating from the ordering of the Udemy course here. This section doesn’t come until later in the course, but I personally thought it made more sense to bring it to the forefront.)

- **Artificial Intelligence (AI)**: AI is a broad term to refer to when a machine emulates human-like intelligence. When I mean human-like intelligence, I mean a range of activities from classifying shapes, speaking verbally in words, recognizing people in images, and many more things that we as humans can do via natural biological processes. AI today largely manifests as advanced statistical processes that we refer to as machine learning, but AI is not necessarily limited to only manifesting as machine learning. Robotics are also considered a form of AI.
- **Machine learning (ML)**: Machine learning is a subset of AI, and much of what we know as AI today manifests as machine learning. Machine learning (ML) is an advanced statistical processes that can learn to generalize patterns in data after being exposed to that data in what we refer to as **model training**. For a simple example, think about the weather. If we wanted to train an AI model to predict tomorrow’s weather, we would feed it lots of data from previous years so that the mathematical algorithm underlying the ML model can learn, “Oh yes, it’s hot in the summer and cold in the winter.”
- **Deep learning (DL)**: When machine learning first came around, it manifested in simpler, structured datasets. (Imagine big Excel spreadsheets.) With things like images or audio files, the means of creating AI models around them began to require many, many more parameters than a standard deep learning model. Because of their emulation of the brain, deep learning models have manifested in what we refer to as **neural networks**. Because these neural networks go so deep on how many parameters they require, we refer to this more nuanced branch of machine learning as deep learning. Part of this distinction also is a bit of a shorthand expression to say, “You’re going to need specialized hardware for this.” A commodity laptop, like your own personal laptop, would struggle to train a deep learning model. Deep learning models often require **graphical processing units (GPUs)**. (Which is why NVIDIA’s stock is doing so well, since they are the world’s most prevalent manufacturer of GPUs!)
- **Generative AI (GenAI)**: Generative AI (or GenAI) is a more recent subset of deep learning that has become popularized since the development of the **Transformer architecture** in 2017. GenAI allows a user to generate some content based on content that the GenAI model has been trained on. For example, you may be familiar with ChatGPT. ChatGPT would be considered a text-based GenAI model that can produce text based on what the user’s input prompt is. (Note: We cover more on prompts and prompt engineering in a later section.)
- **Model training**: As we touched on above, model training is the process in which we expose a mathematical algorithm to a set of data so that it can learn patterns in that data and can be later used to produce probabilistic inferences. If you think about the model’s underlying parameter as “rules”, then model training is having the machine “learn” those “rules” by examining the behavior of the data.
- **Supervised learning**: Supervised learning is a kind of machine learning where we give an ML algorithm a set of data alongside a labelled output serving as the “ground truth” of what we’re looking to predict. For example, if we wanted to train an ML algorithm to predict the cost of the house, we might feed the ML algorithm examples of a bunch of different houses along with their various attributes (e.g. number of rooms, bathrooms) AND the final price the house sold for.
- **Unsupervised learning**: As opposed to supervised learning, unsupervised learning is NOT provided any sort of labelled output and instead asks the mathematical algorithm to make sense of the input data as it sees fit. A classic example of this is customer clustering. For example, you may have a bunch of data about your customers who purchase drinks at a coffee shop, and to determine the behavior of how certain groups buy coffee, you may use a clustering algorithm to cluster like-customers together. You can then examine these like-customer clusters to determine an ideal marketing strategy.
- **Reinforcement learning (RL)**: Reinforcement learning is this idea of providing an AI algorithm a “policy” of how to adhere to a certain goal. The most popular example of reinforcement learning is in self-driving cars. With self-driving cars, a policy is formed to adhere to basically making the car drive safely.
- **Bias**: As you can imagine, if you train an AI model on only a specific kind of data, it is going to generalize only to that specific kind of data. Then, if it sees something that it did not see during training — an anomaly, if you will — then the AI model wouldn’t know how to correctly handle that. We call this bias, and it’s something that we want to avoid for multiple reasons. Chiefly, we want to avoid bias because it doesn’t generalize well to unseen data and thus can produce undesired results.
- **Inference**: After a model has been trained, we then use it in a production context for real world usage. When we make a query to the AI model, it is producing a probabilistic result, not a specifically deterministic result. As such, we refer to these as inferences, so you will often hear “model outputs” and “model inferences” used interchangeably.
- **Model evaluation**: Depending on the AI model, we can use different sorts of metrics to assess the effectiveness of the model. For simpler, tabular models like binary classifiers or regressors, we can use statistical measures as these model evaluation metrics. These include things like F1 score, ROC AUC score, RMSE, confusion matrix, and more. (I'm not delving into what those are because I'm guessing that this exam won't go that specific.) More complex models, on the other hand, are much harder to assess since they often contain billions of parameters that would have to be measured. As such, there's not a particularly "clean" way to evaluate models like LLMs.
- **Reinforcement learning on human feedback (RLHF)**: When a Generative AI model like a large lanugage model (LLM) is trained, it generally doesn't give results that we would necessarily expect. For example, if you ask, "What is the capital of Illinois?", an initially trained LLM might respond, "What is the capital of Indiana?" The reason for this is that LLMs are trained to predict the next probable word based on the data it was trained on, and there's a chance that the most likely next probable words aren't necessarily what you expect. In my simple example above, the results may be that way because maybe the model was trained on historical results of what people have Googled in the past. To achieve the results that we would prefer to see, most LLMs go through a process called reinforcement learning on human feedback (RLHF). This is the process of continuing to refine the model to produce results that we humans would prefer to see. Continuing with our example, an RLHF'd LLM would answer correctly, "The capital of Illinois is Springfield."
- **Features & feature engineering**: When it comes to the data science space, we use the word "feature" in a way that might be foreign to others. Think about your typical Excel spreadsheet, where we have rows and columns. Generally speaking, the way that people organize their data in an Excel spreadsheet is by having each row be an individual record of something, and each column would represent an attribute of that data. We refer to this holistic kind of data as **tabular data**, and those columns we refer to as **features**. Features are essentially synonymous with other words like "data attribute". When using tabular data for AI, we need the data transformed into a numerical format since all AI models work on statistical algorithms. To craft the data (aka features) into something that works for us, we refer to that as **feature engineering**. I share this because you will later come across SageMaker Data Wrangler in this study guide, and it specifically describes itself as "a service for feature engineering."
- **MLOps**: Somewhat akin to the other popular software engineering concept called DevOps (or DevSecOps), this is the process of streamlining your model development process. In addition to streamlining the process of training and deploying your model, MLOps also includes model evaluation metrics to ensure that your model remains effective over time. If this model evaluation falls out of expected performance, an MLOps pipeline may trigger a retrain / redeploy of the model.
    - David's note: You'll want to be careful about how you set up your MLOps structure. In my opinion, you should try to streamline your process as much as possible but NOT particularly automate everything. Namely, I would advise against an automated redeployment after an automated retrain. This is because you really should have a human checking the results of the retrain before doing the redeploy to ensure that the automated retrain was successful.



### Responsible AI
We've actually started to touch on this a little already, but we will carve this out as its own respective section since it is a very important topic. As the name implies, Responsible AI refers to the practices and methodoligies we put in place to ensure that an AI model is being used as intended.

- **Governance & compliance**: As with all other IT products, you want to have a system in place to ensure that people are doing only what they're supposed to be doing. We refer to this as governance, and in the context of Responsible AI, this may mean ensuring that users are only allowed to access / use specific AI services. Generally speaking, AWS manages this goverance using the IAM service. (We cover IAM more in the next section.)
- **The various aspects of Responsible AI**: Responsible AI has several different aspects that we need to take into consideration. These include the following:
    - **Fairness**: Ensuring the AI model isn't biased nor discriminatory
    - **Explainability**: Depending on the type of AI model you choose, you may be able to give a better explanation as to why an AI model made its prediction. As mentioned before, LLMs in particular have very low explainability since they typically have billions of parameters to consider.
    - **Privacy / security**: This is the idea that you are using a user's data in an appropriate way.
    - **Veracity / robustness**: This is the idea that a model can be safely and correctly used even in more "edge case" scenarios.
    - **Governance**: We touched more on this point above already, but it's good to reiterate here.
    - **Safety**: This ensures that the AI model is benefical for society and doesn't cause any undesired / adverse effects.
    - **Controllability**: As the name implies, this is the idea that we as humans can maintain a level of control over what the AI model does so that the AI model is aligned with human intent.
- **Human-Centered Design (HCD) for explainable AI**: This is the concept of considering how to create an AI model to align with a humans' needs. It takes into account all the aspects of Responsible AI as mentioned above.
- **GenAI challenges**: Because GenAI is a new field, there can be a lot of risk to using GenAI models. We already mentioned, for example, that GenAI models are not easily evaluated because they have billions of parameters. Additionally as of August 2024, we are still very much in the early days for regulatory compliance. In the United States, there is very little regulatory compliance around GenAI today; however, many expect that to change over time.
- **Hallucinations**: These are responses generated by GenAI models that seem confident in their correctness but may actually be incorrect. This can be due to a variety of reasons. One major reason is due to the fact that GenAI models are trained at a snapshot in time. For example, if I were to ask a GenAI model for the score of next year's Super Bowl, it may give a confident response, but it would obviously be incorrect since that event has not yet happened. Be extremely careful with hallucinations!!!
- **Model poisoning**: This is the concept of introducing malicious or biased data into a model during its training process. For example, if you trained a GenAI model only on racist text, the model would be "poisoned" to only produce racist responses.
- **Prompt injections**: Prompt injections (or prompt injection attacks) are when people attempt to "override" the instructions provided by a prompt template to produce other results. There are ways to curb prompt injections by using special character delimiters that specifically do not allow input text to override the prompt template.
- **Prompt leaking**: This is the idea that you can draw sensitive information out of a GenAI model by asking it to give that sensitive information. For example, if a GenAI model was trained against data that also included a person's name and Social Security number, it's possible that a malicious user could prompt the model to provide that information to them.
- **Jailbreaking**: This is the concept of prompting a GenAI model in a specific way to get it to operate beyond what it is typically allowed to do. For example, most LLMs have been tuned to NOT tell a user how to make a bomb. By jailbreaking an LLM, you can get it to give you those instructions on how to make bombs.
    - David's note: Please don't try to do this. I know it sounds interesting, but most jailbreaks are caught and squashed by model providers as soon as they're found. Moreover, if you try to jailbreak your company's internal LLM, you will likely get caught and possibly fired.




### Intro to AWS & Cloud Computing
Being a “foundation” level exam, there are a few correlates over to the more general Cloud Practitioner exam. These things are about AWS in general and are not particularly specific to anything related to AI. Many of these services can also be used for non-AI use cases.

- **IAM**: Standing for Identity & Access Management, IAM shows up in all AWS exams, and for good reason! This is the service that enforces who can do what in your AWS account, including non-human resources. Getting your IAM roles and policies in order is of UTMOST importance when enforcing security of your company’s information!!
- **Regions & availability zones**: Remember that all AWS regions have at least 3 availability zones (AZs).
- **Shared Responsibility Model**: AWS is responsible for security “of the cloud” while the customer is responsible for their resources “in the cloud”. For example, AWS is responsible for securing the physical data centers and servers themselves while the customer is responsible for setting things like database credentials to ensure only the right people have access.
- **EC2**: This is AWS's general compute service. You can stand up your own EC2 instance and use it for a wide variety of purposes. You can also set different instance types to include things like more memory (RAM) or use GPUs instead of CPUs. EC2 is also used as the backend for many other services on AWS.
    - David's note: Per that last sentence there, most folks I know choose to use some other service on AWS instead of going with EC2 directly. This is because services like AWS SageMaker (which we cover in depth below) offer some level of abstraction that makes them easier to work with. If you choose to go with EC2 directly, you will be responsible for a lot more than you're probably comfortable with. I personally would not recommend EC2 as your "go to" service unless there is nothing else that fits your need on AWS.
- **AWS Lambda**: This is Amazon's primary "serverless compute" service. By "serverless", we mean that the "behind the scenes" work is completely managed by AWS. Lambda functions are generally intended to be used for smaller, more lightweight tasks.
- **Amazon Macie**: This is a fully managed data security and data privace service that uses machine learning and pattern matching to protect your sensitive data on AWS.
- **AWS Config**: As the name implies, this service stores the configuration information for a variety of other AWS services. You can also use Config to check that other services comply with the configurations set within Config.
- **Amazon Inspector**: This service can run automated security assessments on things like EC2 instances, container images stored in Amazon ECR, and AWS Lambda functions. Findings are stored within AWS Security Hub. It checks for things like package vulnerabilities.
- **AWS CloudTrail**: This service is used to give an indication of who (or which resource) is performing which actions. It's basically an audit trail to manage who is doing what. For example, if you want to know who deployed some model to SageMaker, you could review that here in CloudTrail.
- **AWS Artifact**: This is where AWS stores its various compliance documentation and agreements.
- **AWS Trusted Advisor**: This service gives a high level assessment of your AWS account for the following 6 categories: cost optimization, performance, security, fault tolerance, service limits, and operational excellence. You get a different level support per which AWS plan you have.
- **VPC**: A VPC (virtual private cloud) is a way to ensure on AWS that your resources are isolated from any other organization's resources. Many services, including AWS Bedrock and SageMaker, can be deployed via a VPC and then exposed out using a VPC endpoint.



### AWS Bedrock
This section covers everything specifically related to AWS Bedrock, which is the primary service that AWS uses to house foundation models.

- **Foundation model (FM) vs. large language model (LLM)**: AWS defines a foundation model (FM) differently from a large language model (LLM). Specifically, it sees foundation models as being any GenAI model regardless of modality (e.g. text, audio, image, etc.) whereas LLMs are specifically focused on text generation.
- **Amazon Titan**: While Bedrock is a home for many different types of foundation models, Titan is the one created by AWS directly and offers text generation, text embedding, multimodal embedding, and image generation.
    - David’s note: I would expect this certification to focus more on this particular option in Bedrock than the other options; however, as of August 2024, the Titan models generally do not perform as well as the other options.
- **Pricing**: Pricing is generally based on usage, specifically priced at number of input / output tokens. The pricing may differ from model to model. You can also book a provisioned throughput for a set period of time.
- **Fine tuning**: Bedrock allows a user to fine tune specific models in Bedrock with a customer’s own data. The data must be in a specific format and must be located within S3. Only a select few models in Bedrock are available to be fine tuned.
- **Hyperparameters**: This can be a tricky context to learn, especially when compared to parameters. At the end of the day, a predictive, AI model is a fancy mathematical algorithm with lots of **parameters** that are trained on data during a model training period. (At the start of model training, the parameters are initialized randomly.) Hyperparameters, on the other hand, are external to this mathematical algorithm and are set statically, so they are not adjusted at all during model training. Still, hyperparameters can have a big influence. In the context of this specific exam, what you need to be aware of is that if you want a Generative AI model to behave a certain way, you’ll need to set a few of these hyperparameters. We’ll talk specifically about a few of those later on down in this guide.
- **Evaluation**: You can perform evaluation of a foundational model in Bedrock using your own custom benchmark questions and answers. You can also have humans in your organization perform this evaluation via the Bedrock service.
- **RAG**: RAG is a general concept that has recently come up alongside the GenAI revolution, and it stands for **retrieval augmented generation**. Essentially, it allows a user to get better answers from a foundational model by providing it with custom information, like your company’s own proprietary information. Oftentimes, this tactic is more cost effective yet just as performant as a fine tuning.
- **Bedrock Knowledge Base**: This is Bedrock’s implementation of a RAG service. Basically, you upload your information to S3 and behind the scenes, Bedrock Knowledge Base sets up a **vector database** to convert your information in S3 into vector embeddings stored in this database. By converting the text into vector embeddings, we can perform mathematical calculations to produce a **similarity score** to retrieve the most relevant documents per the user’s query. Options for this vector database backend include:
    - AWS OpenSearch (the most popular option)
    - AWS Aurora
    - MongoDB
    - Redis
    - Pinecone
- **Bedrock Guardrails**: This is the means to protect a company’s information by setting up these “guardrails” to prevent certain information from reaching the foundation model. For example, you might want to ensure that all Social Security numbers are scrubbed before they go to the foundation model for security reasons. Guardrails is designed to do just this. It can also be set to detect things like violence, hateful speech, prompt injection, and more. You can also create multiple Guardrails, each with a varying degree of settings.
- **Bedrock Studio**: This is a UI designed for users to work on their own prompt engineering and other things with the Bedrock models.
- **Agents**: AI agents are this concept that will fulfill a task without having particularly explicit instructions on how to complete that task. This is very different from the standard computer science world, where code is always very explicitly written in a precise, specific way.
    - David’s notes: While I admittedly haven’t tried Bedrock Agents, I have tried to create agents in other fashions using state of the art models. As of August 2024, I do not recommend using agents. Because they do have the “freedom” to do whatever they want, they often get stuck in a weird dead end or in an endless loop. In my opinion, they are currently too unreliable for business use. As technology progresses, I expect my opinion to change on the matter!
- **PartyRock**: PartyRock is actually not a part of AWS Bedrock but is powered by Bedrock on the backend. It's actually a GenAI-powered playground that allows people to test out prompt engineering and more. Users don't have to have an AWS account to use PartyRock.



### Prompt Engineering
This section covers what prompt engineering is and a few tactics for optimization.

- **What is prompt engineering?**: Prompt engineering is the concept of refining your input into a foundation model (FM) to enhance and optimize your desired results. Ideal prompt engineering generally consists of…
    - **Instructions**: What you want the FM to do
    - **Context**: Information that the model may not have been exposed to to enhance results (see RAG above)
    - **Input data**: Any sort of input data
    - **Output format**: What you want the output to be formatted (e.g. “Give me a response in less than 50 words.”)
- **Negative prompting**: This is the concept of prompting to intentionally exclude specific content in the FM’s response.
- **System prompts**: System prompts are a “meta prompt” applied to any user query going forward. System prompts are often used by a chatbot owner to enforce things like friendliness and helpfulness. (Or, if you’re an idiot like me, this is where you tell it to answer like Jar Jar Binks.)
- **Temperature**: This is a value that can be tweaked to control the creativity of the model’s response. Temperature is often adjustable between 0-1 or 0-2. The higher the number, the more creative the response; the lower the number, the more deterministic the response.
    - David’s notes: For the exam, focus on the values ranging from 0-1. Technically speaking, the model can accept 1,000,000 as a value. It’s just that generally speaking, any temperature that exceeds a 1 or 2 starts to become nonsensical.
    - Another David note: For my computer science friends out there, temperature is NOT the same as a seed!! A seed fixes the outputs, while the temperature only makes them more deterministic. That means setting the temperature to zero, you are pretty likely to get the same results but NOT guaranteed like you would be with a seed number.
- **Top P**: Top P is another value that can be set to put a constraint on what probability of most likely words you want to surface in the model response. This value generally ranges between 0 and 1. A low P value (e.g. 0.25) would retrieve only the most likely words, where as a high P value (e.g. 0.99) would be more liberal in its range of likely words, producing a more diverse output. (Diverse in the sense of different kinds of words. If this value is set too high, you can imagine that your output could become incomprehensible!)
- **Top K**: Top K is very similar to Top P, but instead of being a percentage-like value, it’s a fixed value. For example, if you set Top K to equal 10, the model will respond with the top 10 most probable words. Again, a higher Top K generally influences a more creative output.
- **Length**: In the context of large language models, length refers to the number of tokens you want to set as a maximum response. If the model is still going once the length is hit, it halts its process and produces the output.
- **Stop sequences**: Behind the scenes, the model knows to stop producing an output because it will encounter a special “stop” token. Again, we as users of services like ChatGPT don’t see this in the UI, but it is indeed what is going on behind the scenes. Likewise, this field allows you to also use your own stop sequences to serve as that signal for the model to stop producing a response.
- **Zero-shot prompting**: This is the concept of providing a task to an FM without any sort of specific instructions nor examples.
- **Few-shot prompting**: As opposed to zero-shot prompting, this is when you would provide multiple examples of an expected output to help guide the FM toward your desired output. (Note: If you provide a single example, this is also called **one-shot prompting**.)
- **Chain of Thought prompting**: This is the concept of asking the FM to consider its output (“think step by step”) as it is being produced. Oftentimes, people have found that a model is able to provide a better sense of reasoning since it has to explicitly walk itself through each step without necessarily jumping to any conclusions.



### Amazon Q
Generally speaking, Amazon Q is currently AWS’s initiative to bring Generative AI to all aspects of AWS. I honestly expect this to evolve beyond Q over time, but for now, the exam covers the bulleted information below.

- **Available models**: As of today, you may only use models from within AWS Bedrock for the Amazon Q services. This specifically means that using the OpenAI models are not allowed today.
- **Amazon Q Business**: This is a GenAI assistant for your business, such as providing summaries, generating content, and performing routine tasks. In order to make use of this service, you WILL have to expose your company or personal information to the service!! It can connect to 40+ different services (e.g. S3, RDS, Aurora) via data connectors. You can also use 3rd party plugins. Users are managed with IAM.
- **Amazon Q Apps**: This is a subservice of Amazon Q Business. It is designed to allow a user to create a web-based application using natural language.
    - David’s note: I’m sure this service will significantly improve over time, but as of today, I can say with confidence that most companies probably don’t want to use this service. This is because many companies, especially larger corporations, adhere to UX standards, like specific Hex values for colors and the like. Strictly enforcing those UX standards can be a tricky thing to do, and relying on AI to do that today is not the most prudent decision. (Again, I expect my opinion to change over time as the AI technology improves!)
- **Amazon Q Developer**: Amazon Q Developer can answer questions about AWS documentation and resources in your AWS account. (E.g. “List all my AWS Lambda functions.”) Another facet of Amazon Q Developer also works as an AI code companion similar to GitHub Copilot.


### AWS AI Managed Services
AWS provides a number of **managed services** to make things easier on the user to get up and going without having to be an expert with code. For example, instead of having to write Python code to enable a specific AI thing, these managed services generally provide a clean user interface that allows a person to get the results they want without having to know what's going on under the hood. We also refer to this concept as **abstraction**. We've actually covered one of these managed services: AWS Bedrock. This section will cover the other AI managed services that came along before Bedrock.

- **Amazon Comprehend**: This is a natural language processing (NLP) service that is designed to make sense of text in a variety of ways. Comprehend came around well before Bedrock, so while users may be more inclined to use Bedrock today, Comprehend is still good for things like sentiment analysis, information extraction, and more.
- **Amazon Translate**: As the name implies, this services is designed to translate text from one language to another.
- **Amazon Transcribe**: As the name implies, this services is designed to take an audio input (speech) and turn it into a text-based format. It can also redact PII.
- **Amazon Polly**: This is sort of the inverse of Amazon Transcribe. We can provide a set of text and use Amazon Polly to dictate it out in audio form. It supports a number of different languages and voice types.
- **Amazon Rekognition**: This AI service is designed to make sense of things in images.
- **Amazon Forecast**: This service is designed to make sense of time-series data. For example, you might have a bunch of data about customer purchasing patterns from the last several years. You can feed that information to this service, and it will provide a prediction on what future purchasing patterns might look like.
- **Amazon Lex**: This service is designed to create a voice or text-based chatbot. It has integrations with other services including Comprehend.
- **Amazon Personalize**: This service is used to build applications that provide personalized recommendations. It is the same tech that is used to power the general amazon.com.
- **Amazon Textract**: As the name implies (text + extract), this service analyzes things like images and PDFs to extract the text on them.
- **Amazon Kendra**: This is a service that is used to extract information from a corpus of information.
    - David's note: It can be tempting to want to use this as a backend for a RAG system, but Kendra can get very expensive very quickly. I would instead advise using AWS OpenSearch to store your document embeddings. Granted, AWS OpenSearch is not a managed service and requires specialized coding knowledge to enable.
- **Amazon Mechanical Turk**: This is the one service that is not necessarily software-based. This service actually uses real humans on the backend to do some sort of task generally associated to support some later ML activity. For example, if you want to create an image classification AI model, you will need labelled images. You may use this service to have a human force label all these images.
- **Amazon Augmented AI (A2I)**: This service helps to augment the evaluation process to ensure that the AI model is performing correctly. It works by allowing high confidence things to be programmatically evaluated, but anything with low confidence might go to a human workforce for evaluation. You can use either your own employees or the Amazon Mechanical Turk service for the human evaluation piece.
- **AWS DeepRacer**: This service is designed to teach people about reinforcement learning (RL) by training a car to drive itself using just camera feeds. You can actually purchase an RC-like car that will drive itself using your code. It can be a very fun way to learn RL skills.
- **Amazon Comprehend Medical & Transcribe Medical**: These are nuanced versions of these same services that we already mentioned above, specifically for medical use cases. It is HIPAA compliant.
- **AWS Tranium**: This isn't particularly a managed service, but it is a specific service that is used to help users train highly complex AI models like LLMs.
- **AWS Inferentia**: This is very similar to the point above, except where Trainium is designed for training complex models, Inferentia is designed for deploying complex models.



### AWS SageMaker
While we covered all the managed services in the previous section, SageMaker is AWS's "catch all" service for doing AI things at a lower level with far more control. Granted, it also requires an additional skill level since most of how you interact with this service requires an understanding of the Python coding language. (Note: I personally will have a lot of special notes here because this is the service I work in on a daily basis as an ML engineer!)

- **SageMaker Studio**: This is SageMaker's service designed to streamline all portions of the AI model development lifecycle. It is possible to do everything you can do in Studio with other portions of SageMaker, but Studio is designed to optimize the process end-to-end.
    - David's note: I'm not wild about Studio. I get what it's trying to do, but I've found it a bit of a clunky service to work in. Moreover, if you choose to use some "infrastructure as code" tool like Terraform, Studio doesn't work well with that. SageMaker also locks several of the features down below behind the "Studio wall", meaning that you can only access them via SageMaker Studio. I will make note when these other services are locked behind the "Studio wall".
- **SageMaker notebooks**: This is SageMaker's service for managing Jupyter notebooks. Jupyter notebooks are an interface that data scientists / ML engineers use to write Python code to build AI models. It's not required to use Jupyter notebooks, but you can add things like markdown cells to explain what is going on in the code. It is a very nice tool for doing early work like exploratory data analysis. SageMaker notebooks in particular have tight integrations with other AWS services, so you can do things like load data from other AWS services like S3 or Redshift.
    - David's note: You can also technically deploy AI models directly from SageMaker notebooks, but I highly recommend against this. This is not good coding practice, and again, if you use something like Terraform, you wouldn't want to execute these sorts of commands from within a notebook anyway. Don't get me wrong: I really love Jupyter notebooks. It's just that they're designed for a specific purpose, and we shouldn't be using them for things beyond its intended purpose.
- **SageMaker Automatic Model Tuning (AMT)**: We touched on hyperparameters above, and in this context, this service is used to find the ideal hyperparameters to achieve the best results for predictions.
- **SageMaker deployment & inference**: SageMaker offers a variety of subservices to deploy your models for production use. These include real-time endpoints, asynchronous endpoints, batch jobs, and more. There is also a serverless flavor for deployment, but these serverless deployments have a "cold start" problem, meaning that if some time has passed since the last prediction, it might take a little bit for the AI model to start back up to get a new prediction.
    - David's note: I live in this realm. 😂 What I mean by that is that I use these specific subservices all the time in my day job. They are great and can be managed using Terraform.
- **SageMaker Data Wrangler**: This is a subservice of SageMaker that allows a user to prepare both tabular and image data for machine learning.
    - David's note: I personally don't know anybody that uses this service. This is because it's similar to SageMaker Studio in the sense that you can't manage it with Terraform. Instead, other people will use a different subservice of SageMaker called **SageMaker processing jobs** to handle this sort of feature engineering work. In my studies, I have not seen SageMaker processing jobs discussed, but it may or may not come up on the exam.
- **SageMaker Feature Store**: Generally speaking, a feature store is where a data scientist / ML engineer places feature transformations so that for efforts later down the road, another data scientist / ML engineer doesn't have to "reinvent the wheel" to recreate that feature engineering logic. As the name implies, SageMaker also offers this capability in the form of its own SageMaker Feature Store. It is designed to work well with SageMaker Data Wrangler.
    - David's note: We encounter the same issue here as we did with Data Wrangler. This service cannot be managed with Terraform.
- **SageMaker Clarify**: This is a newer subservice designed to compare foundation models together. It attempts to give an explanation to why a foundation model gave a specific response.
    - David's note: This is another service locked behind the "Studio wall".
    - Another David's note: This service honestly sounds a bit sketchy to me. As we touched on above, we noted that it is very difficult to evaluate GenAI models because they often have billions of parameters. To date, the best way I've seen people managing the evaluation of LLMs is by using another LLM. This can cause heartburn for my fellow data scientists out there because basically, you're using one model to validate another model. I could be way off base here, but it seems to me that SageMaker Clarify is doing that same thing. I don't know how else it could do it.
- **SageMaker Ground Truth**: In the AI/ML community, we refer to **ground truth** as this idea of what the true outcome should have been as compared to the predicted outcome of the AI model. The ground truth typically comes in the form of human labellers, so that's what SageMaker Ground Truth is specifically designed to do. You can either use your own workforce or you can leverage the aforementioned AWS Mechanical Turk service.
- **SageMaker JumpStart**: As the name implies here, SageMaker offers a variety of "base level" models that a user can essentially build on top of. For example, you can find certain open source LLMs within this service.
- **SageMaker Model Cards**: To give specific information about the models inside SageMaker JumpStart, you can refer to these SageMaker Model Cards. These Model Cards include information like intended use, training details, and more.
- **SageMaker Role Manager**: This service defines "role personas" to manage these different SageMaker services.
    - David's note: I've genuinely never heard of this, so I'm inclined to think it has something to do with SageMaker Studio. Most people would probably be inclined to leverage IAM for user management over SageMaker Role Manager.
- **SageMaker Model Monitor**: This subservice is designed to help set thresholds around what appropriate use looks like over time.
    - David's note: It may be better now, but when I explored this service in early 2023, the scope of how you could use this subservice was extremely narrow.
- **SageMaker Model Registry**: This is where users can register the models they've built into a single registry for version management and the like.
    - David's note: This cannot be managed with Terraform.
- **SageMaker Pipelines**: These are workflow to help streamline the process of training and deploying an ML model.
    - David's note: Locked behind the "Studio wall." These pipelines cannot be managed with Terraform.
- **SageMaker Canvas**: This is a "no code" service to build models. It can also interact with other AWS AI managed services.
    - David's note: Locked behind the "Studio wall."
- **MLFlow on SageMaker**: MLFlow is actually not an AWS product. It is an open source product used to manage the ML deployment workflow that many people historically have used before AWS started adding all the other subservices we mentioned above. Because people were already used to working in MLFlow, they added an option to use MLFlow in SageMaker.
    - David's note: I actually really like MLFlow, but how it manifests via SageMaker unfortunately cannot be managed with Terraform.